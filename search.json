[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prompts to Table: Structured Data Extraction with Promptflow",
    "section": "",
    "text": "Welcome to the Prompts to table code repo! This contains some code to get started using this workflow for information extraction from medical texts. Check out the pre-print here."
  },
  {
    "objectID": "index.html#what-is-this",
    "href": "index.html#what-is-this",
    "title": "Prompts to Table: Structured Data Extraction with Promptflow",
    "section": "What is this?",
    "text": "What is this?\nThis repository contains code to extract structured data from unstructured text using a prompt-based approach. The goal is to convert free-text medical information into a structured format that can be easily analyzed and processed.\n\n\n\nPrompts to Table Workflow"
  },
  {
    "objectID": "index.html#how-does-it-work",
    "href": "index.html#how-does-it-work",
    "title": "Prompts to Table: Structured Data Extraction with Promptflow",
    "section": "How does it work?",
    "text": "How does it work?\nThe overall workflow is as follows:\n\nDefine Your Target: You create an extraction schema specifying what information you want to extract (e.g., item names, expected labels/values) and entity specific instructions to guide the LLM.\nRun the Pipeline: Using the core pf_batch_run_wrapper function, you point the toolkit to your input data (CSV or JSONL files containing report text) and your schema.\nAutomated Processing: The toolkit leverages Microsoft Promptflow to:\n\nManage connections to LLMs (Azure OpenAI or standard OpenAI-compatible APIs).\nOrchestrate the extraction workflow based on your schema (supporting different patterns like report-level vs.Â specimen-level features or panels).\nProcess reports efficiently in batches with configurable parallelism.\nHandle intermediate data preparation and cleanup.\n\nStructured Output: The final, complex JSON output from Promptflow can be easily converted into a flat, analysis-ready pandas DataFrame using the flatten_outputs utility function."
  },
  {
    "objectID": "index.html#key-concepts",
    "href": "index.html#key-concepts",
    "title": "Prompts to Table: Structured Data Extraction with Promptflow",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nSchemas: The heart of the process. Define your desired output structure, labels, and entity specific LLM guidance. (See the Schema Guide)\nPromptflow Engine: Provides the robust backend for workflow execution, logging, and scalability.\nFlexible Flows: Tailor extraction to different data scopes (report/specimen) and types (feature/panel). *(Learn more in Flow Types\nSimplified Interface: Primarily interact via the main wrapper function. (See the Quickstart or API Reference)\nHelper Utilities: Tools for data prep, result flattening, and even attempting to fix malformed JSON. (Explore the API Reference)"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Prompts to Table: Structured Data Extraction with Promptflow",
    "section": "Getting Started",
    "text": "Getting Started\nThis project utilizes uv for managing the environment and dependencies. To get started, see the Quickstart guide. This will walk you through setting up your environment, installing dependencies, and running the first example.\nThe main orchestration code is located in app/helper_functions and can be imported into notebooks or scripts. The main function to run the pipeline is pf_batch_run_wrapper, which takes care of most of the heavy lifting."
  },
  {
    "objectID": "index.html#prompt-flows",
    "href": "index.html#prompt-flows",
    "title": "Prompts to Table: Structured Data Extraction with Promptflow",
    "section": "Prompt flows",
    "text": "Prompt flows\nThe app directory contains three subdirectories, each representing a specific type of data extraction flow. See the Flow Types guide for more details on how to use these flows."
  },
  {
    "objectID": "index.html#supported-models",
    "href": "index.html#supported-models",
    "title": "Prompts to Table: Structured Data Extraction with Promptflow",
    "section": "Supported Models",
    "text": "Supported Models\nCurrently supported are OpenAI models through Azure, and OpenAI-compatible APIs. Instructions for adding connections is found in guides here."
  },
  {
    "objectID": "docs/guides_tutorials/flows.html",
    "href": "docs/guides_tutorials/flows.html",
    "title": "Prompts to Table",
    "section": "",
    "text": "Page under construction\n\n\n\n\n\n\n\nPrompt flow Structure\nA typical Prompt flow consists of the following files:\n\n**load_[type]_[coverage].py:** This file contains code to load and prepare the input data for the Prompt flow.\n**segment_[type]_[coverage].jinja2:** This file contains a Jinja template for the prompt used in the segmentation step of the Prompt flow. The template includes placeholders for inserting specific information such as labels and custom instructions. The purpose of this LLM call is to segment the relevant text out of the report.\n**standardize_[type]_[coverage].jinja2:** This file contains a Jinja template for the prompt used in the standardization step of the Prompt flow. Similar to the segmentation template, it includes placeholders for specific information. The purpose of this flow is to make standardized data and labels from the segmented text.\n**build_output_[type]_[coverage].py:** This file contains code to combine the outputs from the previous steps, add metadata, and return the final output of the Prompt flow.\nflow.dag.yaml: This file defines the structure and configuration of the Prompt flow. It specifies the inputs, outputs, and nodes of the flow, along with their respective sources, inputs, and connections. If you have the Prompt flow VS Code tool installed you can open these up in a visual editor for a really nice view of the inputs and outputs of each node, the LLM connections being use and their settings, and a nice view of the DAG."
  },
  {
    "objectID": "docs/guides_tutorials/editing_prompts.html",
    "href": "docs/guides_tutorials/editing_prompts.html",
    "title": "Prompts to Table",
    "section": "",
    "text": "Page under construction"
  },
  {
    "objectID": "docs/guides_tutorials/adding_connections.html",
    "href": "docs/guides_tutorials/adding_connections.html",
    "title": "Prompts to Table",
    "section": "",
    "text": "Page under construction"
  },
  {
    "objectID": "docs/code_reference/schemas.html",
    "href": "docs/code_reference/schemas.html",
    "title": "Prompts to Table",
    "section": "",
    "text": "Page under construction"
  },
  {
    "objectID": "docs/code_reference/prep_data.html",
    "href": "docs/code_reference/prep_data.html",
    "title": "Prompts to Table",
    "section": "",
    "text": "Page under construction"
  },
  {
    "objectID": "docs/code_reference/flat_results.html",
    "href": "docs/code_reference/flat_results.html",
    "title": "Prompts to Table",
    "section": "",
    "text": "Page under construction"
  },
  {
    "objectID": "docs/code_reference/run_pf_wrapper.html",
    "href": "docs/code_reference/run_pf_wrapper.html",
    "title": "Prompts to Table",
    "section": "",
    "text": "Page under construction"
  },
  {
    "objectID": "docs/code_reference/utilities.html",
    "href": "docs/code_reference/utilities.html",
    "title": "Prompts to Table",
    "section": "",
    "text": "Page under construction"
  },
  {
    "objectID": "docs/guides_tutorials/basic_usage.html",
    "href": "docs/guides_tutorials/basic_usage.html",
    "title": "Prompts to Table",
    "section": "",
    "text": "Page under construction"
  },
  {
    "objectID": "docs/guides_tutorials/editing_schemas.html",
    "href": "docs/guides_tutorials/editing_schemas.html",
    "title": "Prompts to Table",
    "section": "",
    "text": "Page under construction"
  },
  {
    "objectID": "docs/quickstart.html",
    "href": "docs/quickstart.html",
    "title": "Quickstart",
    "section": "",
    "text": "(if not already installed)\nWindows\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\nmacOS and Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n\n\ngit clone https://github.com/DavidHein96/prompts_to_table.git\ncd prompts_to_table\n\n\n\nRunning this in the root of the repo will create a new venv.\nuv sync\nsource .venv/bin/activate"
  },
  {
    "objectID": "docs/quickstart.html#setting-up-the-environment",
    "href": "docs/quickstart.html#setting-up-the-environment",
    "title": "Quickstart",
    "section": "",
    "text": "(if not already installed)\nWindows\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\nmacOS and Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n\n\ngit clone https://github.com/DavidHein96/prompts_to_table.git\ncd prompts_to_table\n\n\n\nRunning this in the root of the repo will create a new venv.\nuv sync\nsource .venv/bin/activate"
  },
  {
    "objectID": "docs/quickstart.html#add-a-connection",
    "href": "docs/quickstart.html#add-a-connection",
    "title": "Quickstart",
    "section": "Add a connection",
    "text": "Add a connection\nThe first thing to do is come up with a name for your connection. This can be anything you like, but it should be unique to avoid confusion. This is the name that you pass to the main promptflow wrapper function to select a connection at runtime.\nAll connections are stored in the .env file in the root of the project. You can add a connection by adding new lines. All connections need a:\n\nCONNECTION_NAME: The name of the connection. This is the name you will use to refer to this connection in your code.\nAPI_TYPE: The type of API you are using. This can be either openai or azure.\n\n\n\n\n\n\n\nWarning\n\n\n\nAPI Keys: Make sure your .env and API keys are not checked into code repos or shared publically.\n\n\n\nAzure OpenAI\nLetâs say I have two Azure OpenAI deployments, one with GPT-4o and one with GPT-4o mini. To add these we need:\n\nAPI_VERSION: The version of the Azure OpenAI API to use. This usually looks like a date\nAPI_KEY: The API key to use for authentication.\nDEPLOYMENT_NAME: The name of the deployment to use. This is the name you gave your deployment when you created it in Azure.\nAPI_BASE: The base URL for the Azure OpenAI API.\n\nNow with these, we can add each connection. Note that the connection name is the first part of each of the variable names, just in all caps.\nGPT4O_CONNECTION_NAME=gpt4o\nGPT4O_API_VERSION=2024-12-01-preview\nGPT4O_API_KEY=your_api_key\nGPT4O_DEPLOYMENT_NAME=your_deployment_name\nGPT4O_API_BASE=https://your_api_base\nGPT4O_API_TYPE=azure\n\nGPT4O_MINI_CONNECTION_NAME=gpt4o_mini\nGPT4O_MINI_API_VERSION=2024-12-01-preview\nGPT4O_MINI_API_KEY=your_api_key\nGPT4O_MINI_DEPLOYMENT_NAME=your_deployment_name\nGPT4O_MINI_API_BASE=https://your_api_base\nGPT4O_MINI_API_TYPE=azure\n\n\nOpenAI (vllm)\nTo use open weight LLMs, I typically serve them locally with vllm\nFor this, we change the API_TYPE to openai and add the model name, base URL, and API key. The API key is not used for authentication, but it is required by the library. The biggest changes are we now need:\n\nMODEL: The name of the model to use. This is the name you used when you served the model with vllm.\nBASE_URL: This is the address of where the model is being served. This replaces the API_BASE variable.\n\nLets say I want to use Qwen 2.5 72B. If I serve the model with the following command:\nvllm serve \"Qwen/Qwen2.5-72B-Instruct-AWQ\" \\\n    --tensor-parallel 2 \\\n    --max-num-seqs 8 \\\n    --gpu_memory_utilization 0.92 \\\n    --port 9001 \\\n    --max-model-len 4096 \\\n    --dtype float16 \\\n    --quantization=\"awq\"\nThen I can add the connection to the .env file like this:\nQWEN_CONNECTION_NAME=qwen\nQWEN_API_TYPE=openai\nQWEN_MODEL=Qwen/Qwen2.5-72B-Instruct-AWQ\nQWEN_BASE_URL=http://127.0.0.1:9001/v1\nQWEN_API_KEY=EMPTY\nQWEN_DEPLOYMENT_NAME=Qwen2.5-72B-Instruct-AWQ\n\n\n\n\n\n\nNote\n\n\n\nNote that the API_KEY is EMPTY as we are running the model locally, and that the port, 9001, matches the port we are using to serve the model."
  },
  {
    "objectID": "docs/quickstart.html#data-format",
    "href": "docs/quickstart.html#data-format",
    "title": "Quickstart",
    "section": "Data Format",
    "text": "Data Format\nTo use data, we expect either a csv or a jsonl file. The data needs to have the following as columns or keys:\n\nreport_id: An ID (string) that uniquely identifies the report.\nreport_text: The free text of the report.\n\nHere is an example of a csv file:\nreport_id,report_text\nR1,\"This is the first report. It is about something interesting.\"\nR2,\"This is the second report. It is about something else interesting.\"\nHere is an example of a jsonl file:\n{\"report_id\": \"R1\", \"report_text\": \"This is the first report. It is about something interesting.\"}\n{\"report_id\": \"R2\", \"report_text\": \"This is the second report. It is about something else interesting.\"}\n\n\n\n\n\n\nWarning\n\n\n\nPHI: Make sure data containing patient health information is not checked into version control. Also, make sure to follow your institutionâs policies on data sharing and storage."
  },
  {
    "objectID": "docs/quickstart.html#prompt-flows",
    "href": "docs/quickstart.html#prompt-flows",
    "title": "Quickstart",
    "section": "Prompt flows",
    "text": "Prompt flows\nThere are three different prompt flows in the repo. These are:\n\nfeature_report_flow: Entities with one label per report\nfeature_specimen_flow: Entities with one label per specimen\npanel_specimen_flow: Entities for which a panel of tests exist (like IHC/FISH) where we want the specimen, block, test name, and test result for all instances in the report\n\nEach of these subdirectories contains similar files and follows a consistent structure for defining and executing a Prompt flow. The prompts contains basic instructions for medical information extraction and examples of expected output format. They use the Jinja templating engine to allow for re-use across entity types."
  },
  {
    "objectID": "docs/quickstart.html#extraction-schemas",
    "href": "docs/quickstart.html#extraction-schemas",
    "title": "Quickstart",
    "section": "Extraction Schemas",
    "text": "Extraction Schemas\nThe extraction schemas are defined in the schemas subdirectory. These are where we define the labels we want to extract from the report, as well as any entity specific instructions. They are defined in JSONs. The file name of the schema is used as the schema_name in the resulting output. The required top level keys are:\n\nreport_type: I use this as a top level organizer, so something like âpathologyâ or âradiologyâ\nreport_subtypeâ This is more of a sub-organizer, so something like ârccâ or âbreastâ\n\nThen there are keys for each entity type. They do not all have to be present. These keys are:\n\nfeature_report: One label per report\nfeature_specimen: One label per specimen\npanel_specimen: One label per test specimen/block\n\nWithin each of these keys we have the entities, with the top level key being the entity name. The value is a dictionary with keys determined by the entity type.\n\nFeature report & feature specimen\nBoth of these have the same structure. The keys are:\n\nfeature_labels: List of label options\nsegment_feature_instructions: Instructions for the LLM to segment relevant text from the report. These instructions are typically informing what portions of the report contain the likely relevant information.\nstandardize_feature_instructions: Instructions for the LLM to standardize the segmented text. These instructions are typically informing the LLM of extra domain specific knowledge for assigning labels.\n\n\n\nPanel specimen\nThe panel specimen schema is a bit different. The keys are:\n\npanel_test_names: List of label options for names of individual tests\npanel_test_results: List or structured vocabulary of label options for test results\nsegment_1_panel_instructions: Instructions for the LLM to segment relevant text from the report. This step is more broad than the second segment step, which organizes by specimen and block\nsegment_2_panel_instructions: Instructions for the LLM to segment and organize the segmented text by specimen and block.\nstandardize_panel_instructions: These instructions are typically informing the LLM of extra domain specific knowledge for standardizing test names and results.\n\n\n\nExample schema\nBelow is a simple example schema, for extracting a single diagnosis per report, the histology for each specimen, and a handful of IHC tests as a panel.\n{\n    \"report_type\": \"pathology\",\n    \"report_subtype\": \"rcc\",\n    \"feature_report\": {\n        \"diagnosis\": {\n            \"feature_labels\": [\"RCC\", \"Urothelial Carcinoma\", \"Other\"],\n            \"segment_feature_instructions\": \"Segment the relevant text from the report that contains the diagnosis.\",\n            \"standardize_feature_instructions\": \"Standardize the diagnosis to one of the following labels: RCC, Urothelial Carcinoma, Other.\"\n        }\n    },\n    \"feature_specimen\": {\n        \"histology\": {\n            \"feature_labels\": [\"RCC\", \"Urothelial Carcinoma\", \"Other\"],\n            \"segment_feature_instructions\": \"Segment the relevant text from the report that contains the histology.\",\n            \"standardize_feature_instructions\": \"Standardize the histology to one of the following labels: RCC, Urothelial Carcinoma, Other.\"\n        }\n    },\n    \"panel_specimen\": {\n        \"immunohistochemistry\": {\n            \"panel_test_names\": [\"IHC1\", \"IHC2\", \"IHC3\"],\n            \"panel_test_results\": [\"Positive\", \"Negative\", \"Weakly Positive\"],\n            \"segment_1_panel_instructions\": \"Segment the relevant text from the report that contains the IHC tests.\",\n            \"segment_2_panel_instructions\": \"Segment the relevant text from the report that contains the specimen and block information.\",\n            \"standardize_panel_instructions\": \"Standardize the IHC test names and results to one of the following labels: IHC1, IHC2, IHC3 for test names and Positive, Negative, Weakly Positive for test results.\"\n        }\n    }\n}"
  },
  {
    "objectID": "docs/quickstart.html#running-the-flow",
    "href": "docs/quickstart.html#running-the-flow",
    "title": "Quickstart",
    "section": "Running the flow",
    "text": "Running the flow\nGreat, now that weâve covered the basics, letâs try running the flow.\n\n\n\n\n\n\nTip\n\n\n\nCheck out the recommended VS Code plug ins for running the example Jupyter notebook\n\n\nUsing a jupyter notebook, we can run the flow with the following code to import the required libraries and create a Prompt flow client object.\n# First import the PFClient to use the client to interact with PromptFlow\n# Then there are some helper functions that are used to run the flow and get the results\nfrom promptflow.client import PFClient\nfrom app.helper_functions.run_pf_wrapper import pf_batch_run_wrapper\nfrom app.helper_functions.flat_results import flatten_outputs\nfrom app.helper_functions.get_json_outputs import get_json_outputs\n\n# Creating the client\npf_client = PFClient()\nNow we need to decide on our run time parameters. These are:\n\ndata_path (FilePath): Path to either a CSV or JSONL file. The data needs to contain a report_id and report_text column/field.\nschema_path (FilePath): Path to a JSON schema file\nitem_name (str): Name of the item to be processed, should be a key under one of the item types in the schema\nconnection_name (str): Name of the connection to be used as per the .env file\npf_worker_count (int, optional): Number of workers to use for the batch job. Defaults to 4.\nflush_intermediate_data (bool, optional): The intermediate data that is passed to pf.run is deleted by default, but it is interesting to look at and can also be used for debugging. If set to false it will be under app/tmp.\ncsv_to_filter (FilePath, optional): Path to a CSV file to filter the data by report_id. Defaults to None.\n\nNow lets run the flow. A batch job starts and a Prompt flow run object is created.\nflow_result_diagnosis = pf_batch_run_wrapper(\n    pf_client, \n    data_path=\"example_data/input/example_jsonl_data.jsonl\",\n    schema_path=\"app/schemas/pathology_rcc_schema_v12.json\",\n    item_name=\"diagnosis\", \n    connection_name=\"qwen\", \n    pf_worker_count=2\n)\nWe can now use some helper functions to save the output as a CSV and a JSON\n# This handy function extracts and organizes the results in a neat dataframe \nflat_df_diag = flatten_outputs(pf_client=pf_client, flow_result=flow_result_diagnosis)\n\n# This dataframe can then be saved as a csv file\nflat_df_diag.to_csv(\"example_data/output_flat/diagnosis.csv\", index=False)\n\n# This function gets the full JSON outputs from the flow so you can check out the reasoning sections\njson_outs_diag = get_json_outputs(pf_client=pf_client, flow_result=flow_result_diagnosis)\n\n# This can be saved to a JSON file\njson_outs_diag.to_json('example_data/output_json/diagnosis.json', orient='index')"
  }
]